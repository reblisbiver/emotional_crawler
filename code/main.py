"""
å°çº¢ä¹¦/å¾®åšçˆ¬è™« + æƒ…ç»ªåˆ†æä¸»ç¨‹åº
åŠŸèƒ½ï¼šçˆ¬å–å¸–å­ â†’ ä¿å­˜æ–‡æœ¬/å›¾ç‰‡ â†’ æƒ…ç»ªåˆ†æ â†’ å¸¦æ ‡ç­¾å­˜å‚¨
"""

from login_utils import create_chrome_driver, login_xiaohongshu, login_weibo
from crawler_utils import crawl_xiaohongshu, crawl_weibo
from save_utils import save_text_data, save_analyzed_data, save_statistics
from emotion_analyzer import batch_analyze_texts, get_emotion_statistics
from config import CRAWL_CONFIG, EMOTION_CONFIG
import time
import argparse

def main(target_texts=None, target_images=None, analyze_emotion=True, platforms=None):
    """
    ä¸»ç¨‹åºå…¥å£
    :param target_texts: ç›®æ ‡æ–‡æœ¬æ¡æ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰
    :param target_images: ç›®æ ‡å›¾ç‰‡æ•°é‡
    :param analyze_emotion: æ˜¯å¦è¿›è¡Œæƒ…ç»ªåˆ†æ
    :param platforms: è¦çˆ¬å–çš„å¹³å°åˆ—è¡¨ ["xiaohongshu", "weibo"]
    """
    
    target_texts = target_texts or CRAWL_CONFIG["target_texts"]
    target_images = target_images or CRAWL_CONFIG["target_images"]
    platforms = platforms or ["xiaohongshu", "weibo"]
    
    print("=" * 60)
    print("ğŸš€ ç¤¾äº¤åª’ä½“çˆ¬è™« + æƒ…ç»ªåˆ†æç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    print(f"ç›®æ ‡æ–‡æœ¬æ•°é‡ï¼š{target_texts} æ¡")
    print(f"ç›®æ ‡å›¾ç‰‡æ•°é‡ï¼š{target_images} å¼ ")
    print(f"æƒ…ç»ªåˆ†æï¼š{'å¼€å¯' if analyze_emotion else 'å…³é—­'}")
    print(f"ç›®æ ‡å¹³å°ï¼š{', '.join(platforms)}")
    print("=" * 60)
    
    driver = create_chrome_driver()
    driver.implicitly_wait(10)
    print("âœ… æµè§ˆå™¨é©±åŠ¨åˆ›å»ºæˆåŠŸ")
    
    all_text_data = []
    all_image_data = []
    
    try:
        if "xiaohongshu" in platforms:
            print("\n" + "=" * 60)
            print("ğŸ“± å¼€å§‹å°çº¢ä¹¦æµç¨‹ï¼ˆç™»å½•â†’çˆ¬å–ï¼‰")
            print("=" * 60)
            
            login_success = login_xiaohongshu(driver)
            if login_success:
                print("ğŸš€ å°çº¢ä¹¦ç™»å½•å®Œæˆï¼Œå¼€å§‹çˆ¬å–...")
                xhs_text_data, xhs_image_data = crawl_xiaohongshu(driver, target_count=target_texts)
                
                save_text_data("xiaohongshu", xhs_text_data)
                all_text_data.extend(xhs_text_data)
                all_image_data.extend(xhs_image_data)
                
                print(f"âœ… å°çº¢ä¹¦çˆ¬å–å®Œæˆï¼æ–‡æœ¬ {len(xhs_text_data)} æ¡ï¼Œå›¾ç‰‡ {len(xhs_image_data)} å¼ ")
            else:
                print("âŒ å°çº¢ä¹¦ç™»å½•å¤±è´¥ï¼Œè·³è¿‡")
        
        if "weibo" in platforms:
            print("\n" + "=" * 60)
            print("ğŸ“± å¼€å§‹å¾®åšæµç¨‹ï¼ˆæ‰“å¼€æ ‡ç­¾é¡µâ†’ç™»å½•â†’çˆ¬å–ï¼‰")
            print("=" * 60)
            
            if "xiaohongshu" in platforms:
                print("ğŸ”„ æ‰“å¼€å¾®åšä¸“å±æ ‡ç­¾é¡µ...")
                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[-1])
                time.sleep(1)
            
            weibo_login_success = login_weibo(driver)
            if weibo_login_success:
                print("ğŸš€ å¾®åšç™»å½•å®Œæˆï¼Œå¼€å§‹çˆ¬å–...")
                weibo_text_data, weibo_image_data = crawl_weibo(driver, target_count=target_texts)
                
                save_text_data("weibo", weibo_text_data)
                all_text_data.extend(weibo_text_data)
                all_image_data.extend(weibo_image_data)
                
                print(f"âœ… å¾®åšçˆ¬å–å®Œæˆï¼æ–‡æœ¬ {len(weibo_text_data)} æ¡ï¼Œå›¾ç‰‡ {len(weibo_image_data)} å¼ ")
            else:
                print("âŒ å¾®åšç™»å½•å¤±è´¥ï¼Œè·³è¿‡")
        
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–æ±‡æ€»")
        print("=" * 60)
        print(f"æ€»è®¡æ–‡æœ¬ï¼š{len(all_text_data)} æ¡")
        print(f"æ€»è®¡å›¾ç‰‡ï¼š{len(all_image_data)} å¼ ")
        
        if analyze_emotion and EMOTION_CONFIG["analyze_text"] and all_text_data:
            print("\n" + "=" * 60)
            print("ğŸ§  å¼€å§‹æ–‡æœ¬æƒ…ç»ªåˆ†æ...")
            print("=" * 60)
            
            analyzed_texts = batch_analyze_texts(all_text_data, show_progress=True)
            
            save_analyzed_data(analyzed_texts, "texts_with_emotion")
            
            stats = get_emotion_statistics(analyzed_texts)
            save_statistics(stats, "text")
            
            print("\nğŸ“Š æƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡ï¼š")
            for emotion, count in stats["emotion_counts"].items():
                pct = stats["emotion_percentages"][emotion]
                print(f"  {emotion}: {count} æ¡ ({pct}%)")
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµç¨‹æ‰§è¡Œå®Œæˆï¼")
        print("=" * 60)
        print("æ•°æ®ä¿å­˜ä½ç½®ï¼š")
        print("  - åŸå§‹æ–‡æœ¬ï¼š./data/texts/")
        print("  - åŸå§‹å›¾ç‰‡ï¼š./data/images/")
        print("  - åˆ†æç»“æœï¼š./data/analyzed/")
        print("=" * 60)
        
        if EMOTION_CONFIG["analyze_image"]:
            print("\nğŸ’¡ æç¤ºï¼šå›¾ç‰‡æƒ…ç»ªåˆ†æéœ€è¦åœ¨æœ¬åœ°è¿è¡Œ")
            print("   è¯·å°†ä»£ç åŒæ­¥åˆ°æœ¬åœ°åè¿è¡Œï¼š")
            print("   python analyze_images_local.py")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        input("\næŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨...")
        driver.quit()


def run_text_analysis_only():
    """ä»…è¿è¡Œæ–‡æœ¬æƒ…ç»ªåˆ†æï¼ˆä¸å¯åŠ¨çˆ¬è™«ï¼‰"""
    import os
    import json
    
    print("=" * 60)
    print("ğŸ§  æ–‡æœ¬æƒ…ç»ªåˆ†ææ¨¡å¼ï¼ˆåˆ†æå·²æœ‰æ•°æ®ï¼‰")
    print("=" * 60)
    
    text_dir = "./data/texts"
    all_texts = []
    
    for platform in ["xiaohongshu", "weibo"]:
        platform_dir = os.path.join(text_dir, platform)
        if not os.path.exists(platform_dir):
            continue
        
        for filename in os.listdir(platform_dir):
            if filename.endswith(".json") and "å¸¦æƒ…ç»ªæ ‡ç­¾" not in filename:
                filepath = os.path.join(platform_dir, filename)
                with open(filepath, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    all_texts.extend(data)
    
    if not all_texts:
        print("âŒ æœªæ‰¾åˆ°å¾…åˆ†æçš„æ–‡æœ¬æ•°æ®")
        return
    
    print(f"æ‰¾åˆ° {len(all_texts)} æ¡å¾…åˆ†ææ–‡æœ¬")
    
    analyzed_texts = batch_analyze_texts(all_texts, show_progress=True)
    save_analyzed_data(analyzed_texts, "texts_with_emotion")
    
    stats = get_emotion_statistics(analyzed_texts)
    save_statistics(stats, "text")
    
    print("\nğŸ“Š æƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡ï¼š")
    for emotion, count in stats["emotion_counts"].items():
        pct = stats["emotion_percentages"][emotion]
        print(f"  {emotion}: {count} æ¡ ({pct}%)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç¤¾äº¤åª’ä½“çˆ¬è™« + æƒ…ç»ªåˆ†æ")
    parser.add_argument("--texts", type=int, default=None, help="ç›®æ ‡æ–‡æœ¬æ¡æ•°")
    parser.add_argument("--images", type=int, default=None, help="ç›®æ ‡å›¾ç‰‡æ•°é‡")
    parser.add_argument("--no-emotion", action="store_true", help="ä¸è¿›è¡Œæƒ…ç»ªåˆ†æ")
    parser.add_argument("--weibo-only", action="store_true", help="åªçˆ¬å–å¾®åš")
    parser.add_argument("--xhs-only", action="store_true", help="åªçˆ¬å–å°çº¢ä¹¦")
    parser.add_argument("--analyze-only", action="store_true", help="ä»…åˆ†æå·²æœ‰æ•°æ®")
    
    args = parser.parse_args()
    
    if args.analyze_only:
        run_text_analysis_only()
    else:
        platforms = None
        if args.weibo_only:
            platforms = ["weibo"]
        elif args.xhs_only:
            platforms = ["xiaohongshu"]
        
        main(
            target_texts=args.texts,
            target_images=args.images,
            analyze_emotion=not args.no_emotion,
            platforms=platforms
        )
